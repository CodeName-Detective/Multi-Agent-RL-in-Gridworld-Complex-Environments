{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import gymnasium\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "\\text{Batman} & & & & & & & & \\\\\n",
    "\\hline\n",
    " & & &\\text{Alfred} & & & & &\\text{Bane} \\\\\n",
    "\\hline\n",
    " & & & &\\text{Joker} & & & & \\\\\n",
    "\\hline\n",
    " &\\text{Batmobile} & & & & \\text{Alfred} & & & \\\\\n",
    "\\hline\n",
    "& & &\\text{Arkham} & & & &\\text{Court Of Owls} & \\text{Selina}\\\\\n",
    "\\hline\n",
    " & & & & & & & & \\\\\n",
    "\\hline\n",
    " & &\\text{Scarecrow} &\\text{Redbird} & & & & & \\\\\n",
    "\\hline\n",
    " & & & & & &\\text{Alfred} & & \\\\\n",
    "\\hline\n",
    "\\text{Robin} & & & & & & & &\\text{Alfred} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "class GothamCityCooperative(gymnasium.Env):\n",
    "    def __init__(self, max_timesteps):\n",
    "        # Environment Details\n",
    "        self.grid_size = (9,9)\n",
    "        \n",
    "        ## Initializing the Observation Space and Action Space\n",
    "        self.observation_space = spaces.Discrete(self.grid_size[0]* self.grid_size[1])\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.time_step = 0\n",
    "        \n",
    "        ## Actor Positions\n",
    "        self.bat_pos = [0,0]\n",
    "        self.robin_pos = [8,0]\n",
    "        self.selina_pos = (4,8)\n",
    "        \n",
    "        ## Negative Actors\n",
    "        self.joker_pos = (2,4)\n",
    "        self.arkham_pos = (4,3)\n",
    "        self.bane_pos = (1,8)\n",
    "        self.owl_pos = (4,7)\n",
    "        self.scarecrow_pos = (6,2)\n",
    "        \n",
    "        ## Postive Actors\n",
    "        self.batmobile_pos = (3,1)\n",
    "        self.redbird_pos = (6,3)\n",
    "        self.alfred_pos = ((1,3), (3,5), (7,6), (8,8))\n",
    "        # Load Images\n",
    "        self.batman_image = mpimg.imread('images/bat.png')\n",
    "        self.alfred_image = mpimg.imread('images/alfred.jpg')\n",
    "        self.robin_image = mpimg.imread('images/robin.jpg')\n",
    "        self.batmobile_image = mpimg.imread('images/batmobile.png')\n",
    "        self.redbird_image = mpimg.imread('images/red_bird.jpg')\n",
    "        \n",
    "        self.joker_image = mpimg.imread('images/joker.jpg')\n",
    "        self.arkham_image = mpimg.imread('images/arkham_asylum.jpg')\n",
    "        self.bane_image = mpimg.imread('images/bane.jpg')\n",
    "        self.bane_action = mpimg.imread('images/bane_breaks_bats.jpg')\n",
    "        self.owl_image = mpimg.imread('images/court_of_owls.jpg')\n",
    "        self.owl_action = mpimg.imread('images/court_of_owls_attack.png')\n",
    "        self.scarecrow_image = mpimg.imread('images/scare_crow.png')\n",
    "        self.scarecrow_action = mpimg.imread('images/scarecrow_attack.jpeg')\n",
    "        \n",
    "        self.selina_image = mpimg.imread('images/selina.png')\n",
    "        self.gotham_image = mpimg.imread('images/gotham.jpg')\n",
    "        \n",
    "        # Initializing the State of the environment.\n",
    "        self.state = np.zeros(self.grid_size)\n",
    "        self.state[tuple(self.bat_pos)] = 2\n",
    "        self.state[tuple(self.robin_pos)] = 1\n",
    "        self.state[tuple(self.selina_pos)] = 0.5\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.bat_pos = [0,0]\n",
    "        self.robin_pos = [8,0]\n",
    "        self.time_step = 0\n",
    "        \n",
    "        self.state = np.zeros(self.grid_size)\n",
    "        self.state[tuple(self.bat_pos)] = 2\n",
    "        self.state[tuple(self.robin_pos)] = 1\n",
    "        \n",
    "        observation = self.state.flatten()\n",
    "        \n",
    "        info = {}\n",
    "        info['Termination Message'] = 'Not Terminated'\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def get_reward(self, agent_pos, agent_old_pos, agent_name):\n",
    "\n",
    "        if np.array_equal(agent_pos, self.selina_pos):\n",
    "            # Assigining the reward of +20 on reaching the selina position.\n",
    "            return 20\n",
    "        elif np.array_equal(agent_pos, agent_old_pos):\n",
    "            # Assigining the reward of -1 on statying the same position after action.\n",
    "            return -1\n",
    "        elif np.array_equal(agent_pos, self.joker_pos):\n",
    "            # Assigining the reward of -10 on reaching the joker position.\n",
    "            return -10\n",
    "        elif np.array_equal(agent_pos, self.owl_pos):\n",
    "            # Assigining the reward of -7 on reaching the court of owls position.\n",
    "            return -7\n",
    "        elif np.array_equal(agent_pos, self.bane_pos):\n",
    "            # Assigining the reward of -5 on reaching the bane position.\n",
    "            return -5\n",
    "        elif np.array_equal(agent_pos, self.scarecrow_pos):\n",
    "            # Assigining the reward of -5 on reaching the bane position.\n",
    "            return -3\n",
    "        elif agent_pos in self.alfred_pos:\n",
    "            # Assigining the reward of -5 on reaching the Alfred position.\n",
    "            return 10\n",
    "        elif np.array_equal(agent_pos, self.arkham_pos):\n",
    "            # Assigining the reward of -1 on reaching the Arkham Asylum and moving the agent to starting position.\n",
    "            if agent_name == 'batman':\n",
    "                self.bat_pos = [0,0]\n",
    "            elif agent_name == 'robin':\n",
    "                self.robin_pos = [8,0]\n",
    "            return -1\n",
    "        elif np.array_equal(agent_pos, self.batmobile_pos):\n",
    "            # Assigining the reward of -1 on reaching the Arkham Asylum and moving the agent to starting position.\n",
    "            if agent_name == 'batman':\n",
    "                self.bat_pos = list(random.choice(self.alfred_pos))\n",
    "                return 10\n",
    "            elif agent_name == 'robin':\n",
    "                return 0\n",
    "        elif np.array_equal(agent_pos, self.redbird_pos):\n",
    "            # Assigining the reward of -1 on reaching the Arkham Asylum and moving the agent to starting position.\n",
    "            if agent_name == 'batman':\n",
    "                self.bat_pos = list(random.choice(self.alfred_pos))\n",
    "                return 0\n",
    "            elif agent_name == 'robin':\n",
    "                self.robin_pos = list(random.choice(self.alfred_pos))\n",
    "                return 10\n",
    "        else: return 0\n",
    "    \n",
    "    \n",
    "    def step(self, bat_action, robin_action):\n",
    "        reward_bat = 0\n",
    "        reward_robin = 0\n",
    "        bat_truncated = False\n",
    "        robin_truncated = False\n",
    "        self.state = np.zeros(self.grid_size)\n",
    "        print('init_bat_pos', tuple(self.bat_pos) != self.selina_pos)\n",
    "        print('init_robin_pos', tuple(self.robin_pos) != self.selina_pos)\n",
    "        if tuple(self.bat_pos) != self.selina_pos:\n",
    "            bat_old_pos = self.bat_pos.copy()\n",
    "            if bat_action == 0:\n",
    "                # Take Down action.\n",
    "                self.bat_pos[0] += 1\n",
    "            elif bat_action == 1:\n",
    "                # Take Up action.\n",
    "                self.bat_pos[0] -= 1\n",
    "            elif bat_action == 2:\n",
    "                # Take Right action.\n",
    "                self.bat_pos[1] += 1\n",
    "            elif bat_action == 3:\n",
    "                # Take Left action.\n",
    "                self.bat_pos[1] -= 1\n",
    "            \n",
    "            self.bat_pos[0] = np.clip(self.bat_pos[0], 0, self.grid_size[0]-1)\n",
    "            self.bat_pos[1] = np.clip(self.bat_pos[1], 0, self.grid_size[1]-1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            reward_bat = self.get_reward(tuple(self.bat_pos), bat_old_pos, 'batman')\n",
    "            \n",
    "            self.state[tuple(self.bat_pos)] = 2\n",
    "            \n",
    "            if (self.bat_pos[0] >=0) & (self.bat_pos[0] <= self.grid_size[0]-1) & (self.bat_pos[1] >=0)  & (self.bat_pos[1] <= self.grid_size[1]-1):\n",
    "                bat_truncated = True\n",
    "            else:\n",
    "                bat_truncated = False\n",
    "        \n",
    "        if tuple(self.robin_pos) != self.selina_pos:\n",
    "            robin_old_pos = self.robin_pos.copy()\n",
    "            if robin_action == 0:\n",
    "                # Take Down action.\n",
    "                self.robin_pos[0] += 1\n",
    "            elif robin_action == 1:\n",
    "                # Take Up action.\n",
    "                self.robin_pos[0] -= 1\n",
    "            elif robin_action == 2:\n",
    "                # Take Right action.\n",
    "                self.robin_pos[1] += 1\n",
    "            elif robin_action == 3:\n",
    "                # Take Left action.\n",
    "                self.robin_pos[1] -= 1\n",
    "            \n",
    "            self.robin_pos[0] = np.clip(self.robin_pos[0], 0, self.grid_size[0]-1)\n",
    "            self.robin_pos[1] = np.clip(self.robin_pos[1], 0, self.grid_size[1]-1)\n",
    "            \n",
    "            \n",
    "            reward_robin = self.get_reward(tuple(self.robin_pos), robin_old_pos, 'robin')\n",
    "            \n",
    "            self.state[tuple(self.robin_pos)] = 1\n",
    "            \n",
    "            if (self.robin_pos[0] >=0) & (self.robin_pos[0] <= self.grid_size[0]-1) & (self.robin_pos[1] >=0)  & (self.robin_pos[1] <= self.grid_size[1]-1):\n",
    "                robin_truncated = True\n",
    "            else:\n",
    "                robin_truncated = False\n",
    "        else:\n",
    "            reward_robin = 20\n",
    "        \n",
    "        self.state[tuple(self.selina_pos)] = 0.5\n",
    "        observation = self.state.flatten()\n",
    "        \n",
    "        # Updating the time step.\n",
    "        self.time_step += 1\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        # Updating the episode termination status.\n",
    "        if np.array_equal(self.bat_pos, self.robin_pos) and np.array_equal(self.bat_pos, self.selina_pos) and np.array_equal(self.robin_pos, self.selina_pos):\n",
    "            # Goal position is reached.\n",
    "            terminated = True\n",
    "            info['Termination Message'] = 'Goal Position Reached !!!'\n",
    "        elif self.time_step >= self.max_timesteps:\n",
    "            # Maximum time steps reached.\n",
    "            terminated = True\n",
    "            info['Termination Message'] = 'Maximum Time Reached'\n",
    "        else:\n",
    "            # Episode not terminated.\n",
    "            terminated = False\n",
    "            info['Termination Message'] = 'Not Terminated'\n",
    "        print('obs:', observation)\n",
    "        print('robin_pos:', self.robin_pos)\n",
    "        print('bat_pos:', self.bat_pos)\n",
    "            \n",
    "        return observation, reward_bat, reward_robin, terminated, bat_truncated, robin_truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        fig , ax = plt.subplots()\n",
    "        \n",
    "        ax.imshow(self.gotham_image, extent=[0, self.grid_size[1], 0, self.grid_size[0]])\n",
    "        \n",
    "        ax.imshow(self.batman_image, extent=[self.bat_pos[1], self.bat_pos[1] + 1, self.grid_size[0] - self.bat_pos[0] - 1, self.grid_size[0] - self.bat_pos[0]])\n",
    "        ax.imshow(self.robin_image, extent=[self.robin_pos[1], self.robin_pos[1] + 1, self.grid_size[0] - self.robin_pos[0] - 1, self.grid_size[0] - self.robin_pos[0]])\n",
    "        \n",
    "        ax.imshow(self.batmobile_image, extent=[self.batmobile_pos[1], self.batmobile_pos[1] + 1, self.grid_size[0] - self.batmobile_pos[0] - 1, self.grid_size[0] - self.batmobile_pos[0]])\n",
    "        ax.imshow(self.redbird_image, extent=[self.redbird_pos[1], self.redbird_pos[1] + 1, self.grid_size[0] - self.redbird_pos[0] - 1, self.grid_size[0] - self.redbird_pos[0]])\n",
    "        \n",
    "        ax.imshow(self.joker_image, extent=[self.joker_pos[1], self.joker_pos[1] + 1, self.grid_size[0] - self.joker_pos[0] - 1, self.grid_size[0] - self.joker_pos[0]])\n",
    "        ax.imshow(self.arkham_image, extent=[self.arkham_pos[1], self.arkham_pos[1] + 1, self.grid_size[0] - self.arkham_pos[0] - 1, self.grid_size[0] - self.arkham_pos[0]])\n",
    "        \n",
    "        if np.array_equal(self.bat_pos, self.bane_pos) or np.array_equal(self.robin_pos, self.bane_pos):\n",
    "            ax.imshow(self.bane_action, extent=[self.bane_pos[1], self.bane_pos[1] + 1, self.grid_size[0] - self.bane_pos[0] - 1, self.grid_size[0] - self.bane_pos[0]])\n",
    "        else:\n",
    "            ax.imshow(self.bane_image, extent=[self.bane_pos[1], self.bane_pos[1] + 1, self.grid_size[0] - self.bane_pos[0] - 1, self.grid_size[0] - self.bane_pos[0]])\n",
    "        \n",
    "        if np.array_equal(self.bat_pos, self.owl_pos) or np.array_equal(self.robin_pos, self.owl_pos):\n",
    "            ax.imshow(self.owl_action, extent=[self.owl_pos[1], self.owl_pos[1] + 1, self.grid_size[0] - self.owl_pos[0] - 1, self.grid_size[0] - self.owl_pos[0]])\n",
    "        else:\n",
    "            ax.imshow(self.owl_image, extent=[self.owl_pos[1], self.owl_pos[1] + 1, self.grid_size[0] - self.owl_pos[0] - 1, self.grid_size[0] - self.owl_pos[0]])\n",
    "        \n",
    "        if np.array_equal(self.bat_pos, self.scarecrow_pos) or np.array_equal(self.robin_pos, self.scarecrow_pos):\n",
    "            ax.imshow(self.scarecrow_image, extent=[self.scarecrow_pos[1], self.scarecrow_pos[1] + 1, self.grid_size[0] - self.scarecrow_pos[0] - 1, self.grid_size[0] - self.scarecrow_pos[0]])\n",
    "        else:\n",
    "            ax.imshow(self.scarecrow_image, extent=[self.scarecrow_pos[1], self.scarecrow_pos[1] + 1, self.grid_size[0] - self.scarecrow_pos[0] - 1, self.grid_size[0] - self.scarecrow_pos[0]])\n",
    "        \n",
    "        ax.imshow(self.selina_image, extent=[self.selina_pos[1], self.selina_pos[1] + 1, self.grid_size[0] - self.selina_pos[0] - 1, self.grid_size[0] - self.selina_pos[0]])\n",
    "        \n",
    "        \n",
    "        \n",
    "        for pos in self.alfred_pos:\n",
    "            ax.imshow(self.alfred_image, extent=[pos[1], pos[1] + 1, self.grid_size[0] - pos[0] - 1, self.grid_size[0] - pos[0]])\n",
    "        ax.set_xlim(0, self.grid_size[1])\n",
    "        ax.set_ylim(0, self.grid_size[0])\n",
    "        \n",
    "        ax.axis('off')\n",
    "        # Save rendered image directly to a buffer\n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(buf, format='png', dpi=fig.dpi, bbox_inches='tight', pad_inches=0)\n",
    "        buf.seek(0)\n",
    "\n",
    "        # Convert buffer to PIL Image and then to RGB array\n",
    "        image = Image.open(buf)\n",
    "        rgb_image = np.array(image.convert('RGB'))\n",
    "        \n",
    "        plt.close(fig)  # Close the figure to prevent it from being displayed\n",
    "        return rgb_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GothamCityCooperative(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "  def __init__(self, env):\n",
    "   \n",
    "    self.env = env\n",
    "    self.observation_space = env.observation_space\n",
    "    self.action_space = env.action_space\n",
    "  \n",
    "  def step(self: 'RandomAgent', state: np.ndarray) -> int:\n",
    "\n",
    "    return np.random.choice(self.action_space.n), np.random.choice(self.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "terminated, truncated = False, False\n",
    "\n",
    "env.render()\n",
    "\n",
    "def get_action(action):\n",
    "  # Mapping action number to action\n",
    "  if action == 0:\n",
    "    action_took = 'Down'\n",
    "  elif action == 1:\n",
    "    action_took = 'Up'\n",
    "  elif action == 2:\n",
    "    action_took = 'Right'\n",
    "  elif action == 3:\n",
    "    action_took = 'Left'\n",
    "  \n",
    "  return action_took\n",
    "\n",
    "# Continue through the episode untill we reach termination state\n",
    "while not terminated:\n",
    "  # Agent deciding on what action to choose.\n",
    "  action1, action2 = agent.step(obs)\n",
    "  \n",
    "  \n",
    "  obs, reward_bat, reward_robin, terminated, truncated_bat, truncated_robin, info = env.step(action1, action2)\n",
    "  rgb_array = env.render()\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.imshow(rgb_array)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "    \n",
    "  time.sleep(0.0001)\n",
    "  clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decentralized Q - Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class DecentralizedQLearningAgent:\n",
    "  \"\"\"\n",
    "  Define an RL agent which follows an epsilon greedy algorithm throughout the training.\n",
    "  It uses Q-Learning algorithm to update Q-values.\n",
    "  \"\"\"\n",
    "  def __init__(self: 'DecentralizedQLearningAgent', env: 'gymnasium.Env', learning_rate: 'float', discount_factor: 'float') -> None:\n",
    "    \"\"\"Initializing Epsilon Greedy Agent\n",
    "\n",
    "    Args:\n",
    "        env (gymnasium.Env): object of Grid Environment.\n",
    "        learning_rate (float): Learning rate used in SARSA algorithm.\n",
    "        discount_factor (float): Discount factor to quantify the importance of future reward.\n",
    "    \"\"\"\n",
    "    self.env = env\n",
    "    self.observation_space = env.observation_space\n",
    "    self.action_space = env.action_space\n",
    "    self.learning_rate = learning_rate\n",
    "    self.discount_factor = discount_factor\n",
    "    \n",
    "    # Initiating the Q-table with all Zeros.\n",
    "    self.q_table = np.zeros((2, self.observation_space.n, self.action_space.n))\n",
    "\n",
    "\n",
    "  def step(self: 'DecentralizedQLearningAgent', state_bat: int, state_robin: int, epsilon: float) -> int:\n",
    "    \"\"\"Given the current state and probability of choosing a random action we will\n",
    "    provide the action we should choose.\n",
    "\n",
    "    Args:\n",
    "        state (int): Current location of the agent in the environment.\n",
    "        epsilon (float): Probability of taking random action.\n",
    "\n",
    "    Returns:\n",
    "        action (int): Action represented as number; (0: Down, 1: Up, 2: Right, 3: Left)\n",
    "    \"\"\"\n",
    "    # Epsilon-Greedy Action Selection\n",
    "    random_number = np.random.rand()\n",
    "\n",
    "    if random_number <= epsilon:\n",
    "        action1 = self.env.action_space.sample()\n",
    "        action2 = self.env.action_space.sample()\n",
    "    else:\n",
    "        action1 = np.argmax(self.q_table[0, state_bat,:])\n",
    "        action2 = np.argmax(self.q_table[1, state_robin,:])\n",
    "    \n",
    "    return action1, action2\n",
    "  \n",
    "  def update_qvalue(self: 'DecentralizedQLearningAgent', agent_name: str, current_state: int, current_action: int, \n",
    "                    reward: int, future_state: int) -> None:\n",
    "    \"\"\"Update the Q value based on the Q-Learning algorithm\n",
    "\n",
    "    Args:\n",
    "        current_state (int): Current State represented as integer\n",
    "        current_action (int): Current action represented as number; (0: Down, 1: Up, 2: Right, 3: Left)\n",
    "        reward (int): Immediate reward that was recieved after taking the current action.\n",
    "        future_state (int): Future State represented as integer\n",
    "    \"\"\"\n",
    "    # Q-Learning update Q table\n",
    "    if agent_name == 'batman':\n",
    "        agent_id = 0\n",
    "    elif agent_name == 'robin':\n",
    "        agent_id = 1\n",
    "    print(agent_name)\n",
    "    print(future_state)\n",
    "    print(self.q_table[agent_id, future_state].shape)\n",
    "    self.q_table[agent_id, current_state, current_action] = self.q_table[agent_id, current_state, current_action] + self.learning_rate * (reward \n",
    "                                                                                                    + self.discount_factor * np.max(self.q_table[agent_id, future_state, :]) \n",
    "                                                                                                    - self.q_table[agent_id, current_state, current_action])\n",
    "    print(self.q_table[agent_id, current_state, current_action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state(observation, agent_id):\n",
    "    \"\"\"\n",
    "    Extracts the state from the observation for a specific agent.\n",
    "    \n",
    "    Args:\n",
    "        observation (np.array): The observation array from the environment.\n",
    "        agent_id (int): The identifier for the agent (e.g., 2 for batman, 1 for robin).\n",
    "\n",
    "    Returns:\n",
    "        state: The extracted state for the agent.\n",
    "    \"\"\"\n",
    "    state = np.where(observation == agent_id)[0]\n",
    "    if len(state) == 0:\n",
    "        return None \n",
    "    else:\n",
    "        return state[0]\n",
    "\n",
    "def is_valid_state(state):\n",
    "    \"\"\"\n",
    "    Checks if the provided state is valid.\n",
    "    \n",
    "    Args:\n",
    "        state: The state to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the state is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    return state is not None\n",
    "\n",
    "def handle_invalid_state():\n",
    "    \"\"\"\n",
    "    Provides a fallback for invalid states.\n",
    "\n",
    "    Returns:\n",
    "        state: A default or fallback state.\n",
    "    \"\"\"\n",
    "    default_state = 0 \n",
    "    return default_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decentralized_q_learning_learning_loop(env: 'gymnasium.Env',learning_rate: float, discount_factor: float, episodes: int,\n",
    "                        min_epsilon_allowed: float, initial_epsilon_value: float) -> tuple['decentralized_q_learning_learning_loop', list, list]:\n",
    "  \"\"\"Learning loop train Agent to reach GOAL state in the environment using Q-Learning Algorithm.\n",
    "\n",
    "  Args:\n",
    "      env (gymnasium.Env): object of Grid Environment.\n",
    "      learning_rate (float): Learning rate used in SARSA algorithm\n",
    "      discount_factor (float): Discount factor to quantify the importance of future reward.\n",
    "      episodes (int): Number of episodes we should train.\n",
    "      min_epsilon_allowed (float): Minimum epsilon that we should reach by the end of the training.\n",
    "      initial_epsilon_value (float): Initial epsilon that we should use while starting the learning. \n",
    "\n",
    "  Returns:\n",
    "      tuple[DecentralizedQLearningAgent, list, list]: Returns a tuple containing agent,\n",
    "                                                  cumulative rewards across episodes,\n",
    "                                                  epsilon used across episodes respectively.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Agent.\n",
    "  agent = DecentralizedQLearningAgent(env, learning_rate=learning_rate, discount_factor = discount_factor)\n",
    "  \n",
    "  # Initiating epsilon values.\n",
    "  epsilon = initial_epsilon_value\n",
    "  min_epsilon_allowed = min_epsilon_allowed\n",
    "  \n",
    "  # Calculating Epsilon Decay factor. \n",
    "  epsilon_decay_factor = np.power(min_epsilon_allowed/epsilon, 1/episodes)\n",
    "  \n",
    "  # Initiating list to store rewards and epsilons we use across episodes\n",
    "  reward_bat_across_episodes = []\n",
    "  reward_robin_across_episodes = []\n",
    "  \n",
    "  epsilons_across_episodes = []\n",
    "  \n",
    "  # Iterating over Episodes.\n",
    "  for _ in range(episodes):\n",
    "    # Resetting the environment.\n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    \n",
    "    # Fectcing Current State and Current Action details.\n",
    "    try:\n",
    "      current_state_bat = np.where(obs == 2)\n",
    "    except Exception as e:\n",
    "      print(\"An error occurred:\", e)\n",
    "    \n",
    "    #current_state_robin\n",
    "    try:\n",
    "      current_state_robin = np.where(obs == 1)\n",
    "    except Exception as e:\n",
    "      print(\"An error occurred:\", e)\n",
    "      \n",
    "    current_action_bat, current_action_robin = agent.step(current_state_bat, current_state_robin, epsilon)\n",
    "    \n",
    "    agent.step(state_bat =  current_state_bat, state_robin = current_state_robin, epsilon = epsilon)\n",
    "    \n",
    "    reward_per_episode_bat = 0\n",
    "    reward_per_episode_robin = 0\n",
    "    \n",
    "    epsilons_across_episodes.append(epsilon)\n",
    "    \n",
    "    # Iterating over an epsidoe untill termination status is reached.\n",
    "    while not terminated:\n",
    "      obs, reward_bat, reward_robin, terminated, init_bat_pos, init_robin_pos, info = env.step(current_action_bat, current_action_robin)\n",
    "\n",
    "      # Process new observations to get future states\n",
    "      future_state_bat = process_state(obs, 2)  # 2 represents batman\n",
    "      future_state_robin = process_state(obs, 1)  # 1 represents robin\n",
    "\n",
    "      # Ensure the future states are valid\n",
    "      if not is_valid_state(future_state_bat):\n",
    "          future_state_bat = handle_invalid_state()\n",
    "\n",
    "      if not is_valid_state(future_state_robin):\n",
    "          future_state_robin = handle_invalid_state()\n",
    "\n",
    "      # Updating cumulative rewards for the episode\n",
    "      reward_per_episode_bat += reward_bat\n",
    "      reward_per_episode_robin += reward_robin\n",
    "\n",
    "      # Decide the next action based on the future state\n",
    "      future_action_bat, future_action_robin = agent.step(future_state_bat, future_state_robin, epsilon)\n",
    "\n",
    "      # Updating Q values\n",
    "      agent.update_qvalue(\"batman\", current_state_bat, current_action_bat, reward_bat, future_state_bat)\n",
    "      agent.update_qvalue(\"robin\", current_state_robin, current_action_robin, reward_robin, future_state_robin)\n",
    "\n",
    "      # Updating current states and actions for the next iteration\n",
    "      current_state_bat = future_state_bat\n",
    "      current_action_bat = future_action_bat\n",
    "      current_state_robin = future_state_robin\n",
    "      current_action_robin = future_action_robin\n",
    "      \n",
    "      # Fetching future state and future reward.\n",
    "      try:\n",
    "        future_state_bat = np.where(obs == 2)\n",
    "        print('reward_bat',reward_bat)\n",
    "        print(future_state_bat[0])\n",
    "        print(future_state_bat[0].shape[0] == 0)\n",
    "        if reward_bat ==  20:\n",
    "          future_state_bat = (np.array([44]),)\n",
    "        elif future_state_bat[0].shape[0] == 0:\n",
    "          future_state_bat = np.where(obs == 1)\n",
    "          print('future_state_bat', future_state_bat)\n",
    "      except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "      \n",
    "      try:\n",
    "        future_state_robin = np.where(obs == 1)\n",
    "        print('reward_robin',reward_robin)\n",
    "        print(future_state_robin[0])\n",
    "        print(future_state_robin[0].shape[0] == 0)\n",
    "        if reward_robin ==  20:\n",
    "          future_state_robin = (np.array([44]),)\n",
    "        elif future_state_robin[0].shape[0] == 0:\n",
    "          future_state_robin = np.where(obs == 2)\n",
    "          print('future_state_robin', future_state_robin)\n",
    "      except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    \n",
    "    # Decaying Epsilon\n",
    "    epsilon = epsilon_decay_factor*epsilon\n",
    "    reward_bat_across_episodes.append(reward_per_episode_bat)\n",
    "    reward_robin_across_episodes.append(reward_per_episode_robin)\n",
    "\n",
    "  return agent, reward_bat_across_episodes, reward_robin_across_episodes, epsilons_across_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GothamCityCooperative(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, reward_bat_across_episodes, reward_robin_across_episodes, epsilons_across_episodes = decentralized_q_learning_learning_loop(env,learning_rate = 0.5, discount_factor = 0.99, episodes = 1000, min_epsilon_allowed = 0.01, initial_epsilon_value = 1)\n",
    "\n",
    "with open('decentralized_qlearning.pkl', 'wb') as f:\n",
    "    pickle.dump(agent, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_agents(env, q_tables, num_episodes=10):\n",
    "    total_rewards_bat_eval = []\n",
    "    total_rewards_robin_eval = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        states = [process_state(obs, 2), process_state(obs, 1)]  # 2 and 1 are IDs for Batman and Robin\n",
    "        done = False\n",
    "        total_reward_bat = 0\n",
    "        total_reward_robin = 0\n",
    "\n",
    "        while not done:\n",
    "            # Choose actions greedily\n",
    "            actions = [np.argmax(q_tables[i][states[i]]) if states[i] >= 0 else env.action_space.sample() for i in range(len(states))]\n",
    "\n",
    "            # Environment step with unpacked actions\n",
    "            obs, reward_bat, reward_robin, done, _ = env.step(*actions)\n",
    "\n",
    "            # Update states\n",
    "            next_states = [process_state(obs, 2), process_state(obs, 1)]\n",
    "            states = next_states\n",
    "\n",
    "            total_reward_bat += reward_bat\n",
    "            total_reward_robin += reward_robin\n",
    "\n",
    "        total_rewards_bat_eval.append(total_reward_bat)\n",
    "        total_rewards_robin_eval.append(total_reward_robin)\n",
    "\n",
    "    return total_rewards_bat_eval, total_rewards_robin_eval\n",
    "\n",
    "# Plotting the evaluation results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(eval_rewards_bat, label='Batman')\n",
    "plt.plot(eval_rewards_robin, label='Robin')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Evaluation: Total Rewards per Episode')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(10), eval_rewards_bat, label='Batman', alpha=0.6)\n",
    "plt.bar(range(10), eval_rewards_robin, label='Robin', alpha=0.6)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Evaluation: Rewards Comparison per Episode')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the NumPy array\n",
    "numpy_array = np.array([2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "                        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "                        0., 0., 0., 0., 0., 0., 0., 0., 0.5, 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "                        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "                        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "np.where(numpy_array == 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-Related Q-Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def initialize_q_tables(env, num_agents):\n",
    "    q_tables = [np.zeros((env.observation_space.n, env.action_space.n)) for _ in range(num_agents)]\n",
    "    return q_tables\n",
    "\n",
    "def choose_action(q_table, state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        if isinstance(state, dict):\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            state_idx = int(np.argmax(state))\n",
    "            return np.argmax(q_table[state_idx])\n",
    "\n",
    "def update_q_values(q_tables, states, actions, rewards, next_states, alpha, gamma):\n",
    "    ce_strategy = [0 for _ in q_tables]\n",
    "\n",
    "    for q_table, state, action, reward, next_state in zip(q_tables, states, actions, rewards, next_states):\n",
    "        if isinstance(state, dict):\n",
    "            state_idx = -1 \n",
    "        else:\n",
    "            state_idx = int(np.argmax(state))\n",
    "\n",
    "        action_idx = int(action)\n",
    "\n",
    "        q_table[state_idx, action_idx] = q_table[state_idx, action_idx] + alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state_idx, action_idx])\n",
    "        \n",
    "def process_state(observation, agent_id):\n",
    "    \"\"\"\n",
    "    Convert the observation to a unique integer state index.\n",
    "    Adjust this based on the specifics of the environment's observation space.\n",
    "    \"\"\"\n",
    "    if isinstance(observation, tuple):\n",
    "        for obs in observation:\n",
    "            if hasattr(obs, 'flatten'):\n",
    "                observation_1d = obs.flatten()\n",
    "                agent_indices = np.where(observation_1d == agent_id)[0]\n",
    "                if agent_indices.size > 0:\n",
    "                    return int(agent_indices[0])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return -1\n",
    "    \n",
    "max_timesteps = 100\n",
    "env = GothamCityCooperative(max_timesteps)\n",
    "num_agents = 2\n",
    "q_tables = initialize_q_tables(env, num_agents)\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 1.0  # Initial epsilon\n",
    "episodes = 1000\n",
    "\n",
    "total_rewards_bat = []\n",
    "total_rewards_robin = []\n",
    "epsilon_values = []\n",
    "\n",
    "batman_id = 2 \n",
    "robin_id = 1\n",
    "\n",
    "for episode in range(episodes):\n",
    "    obs = env.reset()\n",
    "    states = env.reset()\n",
    "    done = None\n",
    "    total_reward_bat = 0\n",
    "    total_reward_robin = 0\n",
    "\n",
    "    while not done:\n",
    "        if states is not None:\n",
    "            actions = [choose_action(q_table, state, epsilon) for q_table, state in zip(q_tables, states)]\n",
    "        else:\n",
    "            actions = [env.action_space.sample() for _ in range(num_agents)]\n",
    "\n",
    "        state_idx = int(np.argmax(state))\n",
    "        step_output = env.step(*actions)\n",
    "        obs, reward_bat, reward_robin, done = step_output[:4]\n",
    "\n",
    "        if states is not None:\n",
    "            next_states = [process_state(obs, batman_id), process_state(obs, robin_id)]\n",
    "            update_q_values(q_tables, states, actions, [reward_bat, reward_robin], next_states, alpha, gamma)\n",
    "            states = next_states\n",
    "\n",
    "        total_reward_bat += reward_bat\n",
    "        total_reward_robin += reward_robin\n",
    "\n",
    "    total_rewards_bat.append(total_reward_bat)\n",
    "    total_rewards_robin.append(total_reward_robin)\n",
    "    epsilon_values.append(epsilon)\n",
    "    epsilon = max(epsilon * 0.99, 0.01)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(total_rewards_bat, label='Batman')\n",
    "plt.plot(total_rewards_robin, label='Robin')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Rewards per Episode')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epsilon_values)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title('Epsilon Decay over Episodes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agents(env, q_tables, num_episodes=10):\n",
    "    total_rewards_bat_eval = []\n",
    "    total_rewards_robin_eval = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        states = [process_state(obs, batman_id), process_state(obs, robin_id)]\n",
    "        done = None\n",
    "        total_reward_bat = 0\n",
    "        total_reward_robin = 0\n",
    "\n",
    "        while not done:\n",
    "            actions = [np.argmax(q_table[state]) if state >= 0 else env.action_space.sample() for q_table, state in zip(q_tables, states)]\n",
    "            obs, reward_bat, reward_robin, done = env.step(*actions)[:4]\n",
    "            next_states = [process_state(obs, batman_id), process_state(obs, robin_id)]\n",
    "            states = next_states\n",
    "\n",
    "            total_reward_bat += reward_bat\n",
    "            total_reward_robin += reward_robin\n",
    "\n",
    "        total_rewards_bat_eval.append(total_reward_bat)\n",
    "        total_rewards_robin_eval.append(total_reward_robin)\n",
    "\n",
    "    return total_rewards_bat_eval, total_rewards_robin_eval\n",
    "\n",
    "eval_rewards_bat, eval_rewards_robin = evaluate_agents(env, q_tables, num_episodes=10)\n",
    "\n",
    "# Plotting the evaluation results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(eval_rewards_bat, label='Batman')\n",
    "plt.plot(eval_rewards_robin, label='Robin')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Evaluation: Total Rewards per Episode')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(10), eval_rewards_bat, label='Batman', alpha=0.6)\n",
    "plt.bar(range(10), eval_rewards_robin, label='Robin', alpha=0.6)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Evaluation: Rewards Comparison per Episode')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        return probabilities\n",
    "\n",
    "def reinforce(env, policy_batman, policy_robin, episodes, gamma=0.99):\n",
    "    optimizer_batman = torch.optim.Adam(policy_batman.parameters(), lr=0.01)\n",
    "    optimizer_robin = torch.optim.Adam(policy_robin.parameters(), lr=0.01)\n",
    "    \n",
    "    total_rewards_batman = []\n",
    "    total_rewards_robin = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        state = np.array(state).flatten()\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        state = np.array(state).flatten()\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "        log_probs_batman = []\n",
    "        log_probs_robin = []\n",
    "        rewards_batman = []\n",
    "        rewards_robin = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action_probs_batman = policy_batman(state_tensor)\n",
    "            action_probs_robin = policy_robin(state_tensor)\n",
    "\n",
    "            distribution_batman = Categorical(action_probs_batman)\n",
    "            distribution_robin = Categorical(action_probs_robin)\n",
    "\n",
    "            action_batman = distribution_batman.sample()\n",
    "            action_robin = distribution_robin.sample()\n",
    "\n",
    "            output = env.step(action_batman.item(), action_robin.item())\n",
    "\n",
    "            next_state, reward_batman, reward_robin, done, *extra = env.step(action_batman.item(), action_robin.item())\n",
    "            next_state = np.array(next_state).flatten()\n",
    "            state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            \n",
    "            log_prob_batman = distribution_batman.log_prob(action_batman)\n",
    "            log_prob_robin = distribution_robin.log_prob(action_robin)\n",
    "\n",
    "            log_probs_batman.append(log_prob_batman)\n",
    "            log_probs_robin.append(log_prob_robin)\n",
    "            rewards_batman.append(reward_batman)\n",
    "            rewards_robin.append(reward_robin)\n",
    "\n",
    "        update_policy(optimizer_batman, log_probs_batman, rewards_batman, gamma)\n",
    "        update_policy(optimizer_robin, log_probs_robin, rewards_robin, gamma)\n",
    "\n",
    "        total_rewards_batman.append(sum(rewards_batman))\n",
    "        total_rewards_robin.append(sum(rewards_robin))\n",
    "\n",
    "        print(f\"Episode {episode} finished with total reward Batman: {sum(rewards_batman):.2f}, Robin: {sum(rewards_robin):.2f}\")\n",
    "\n",
    "    plot_rewards(total_rewards_batman, total_rewards_robin)\n",
    "\n",
    "\n",
    "def update_policy(optimizer, log_probs, rewards, gamma):\n",
    "    discounted_rewards = [gamma ** i * r for i, r in enumerate(rewards)]\n",
    "    policy_loss = -torch.sum(torch.stack(log_probs) * torch.tensor(discounted_rewards))\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def plot_rewards(total_rewards_batman, total_rewards_robin):\n",
    "\n",
    "# Plotting the results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(total_rewards_batman, label='Batman')\n",
    "    plt.plot(total_rewards_robin, label='Robin')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Total Rewards per Episode')\n",
    "    plt.legend()\n",
    "\n",
    "input_dim = 81\n",
    "hidden_dim = 128 \n",
    "output_dim = env.action_space.n \n",
    "\n",
    "policy_batman = PolicyNetwork(input_dim, hidden_dim, output_dim)\n",
    "policy_robin = PolicyNetwork(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "env = GothamCityCooperative(max_timesteps=100)\n",
    "input_dim = env.observation_space.n\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "policy_batman.apply(init_weights)\n",
    "policy_robin.apply(init_weights)\n",
    "\n",
    "reinforce(env, policy_batman, policy_robin, episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "def evaluate_agents(env, policy_batman, policy_robin, num_episodes=10):\n",
    "    total_rewards_bat_eval = []\n",
    "    total_rewards_robin_eval = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        output = env.reset() \n",
    "        state = output[0] if isinstance(output, tuple) else output\n",
    "        \n",
    "        state = np.array(state).flatten()\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        done = False\n",
    "        total_reward_bat = 0\n",
    "        total_reward_robin = 0\n",
    "\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action_probs_batman = policy_batman(state_tensor)\n",
    "                action_probs_robin = policy_robin(state_tensor)\n",
    "            \n",
    "            distribution_batman = Categorical(action_probs_batman)\n",
    "            distribution_robin = Categorical(action_probs_robin)\n",
    "            action_batman = distribution_batman.sample()\n",
    "            action_robin = distribution_robin.sample()\n",
    "\n",
    "            next_state, reward_bat, reward_robin, done, *extra = env.step(action_batman.item(), action_robin.item())\n",
    "            next_state = np.array(next_state).flatten()\n",
    "            state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "            total_reward_bat += reward_bat\n",
    "            total_reward_robin += reward_robin\n",
    "\n",
    "        total_rewards_bat_eval.append(total_reward_bat)\n",
    "        total_rewards_robin_eval.append(total_reward_robin)\n",
    "\n",
    "    return total_rewards_bat_eval, total_rewards_robin_eval\n",
    "\n",
    "eval_rewards_bat, eval_rewards_robin = evaluate_agents(env, policy_batman, policy_robin, num_episodes=10)\n",
    "\n",
    "# Plotting the evaluation results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(eval_rewards_bat, label='Batman')\n",
    "plt.plot(eval_rewards_robin, label='Robin')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Evaluation: Total Rewards per Episode')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(10), eval_rewards_bat, label='Batman', alpha=0.6)\n",
    "plt.bar(range(10), eval_rewards_robin, label='Robin', alpha=0.6)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Evaluation: Rewards Comparison per Episode')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "class GothamCityCooperative(gymnasium.Env):\n",
    "    def __init__(self, max_timesteps):\n",
    "        # Environment Details\n",
    "        self.grid_size = (9,9)\n",
    "        \n",
    "        ## Initializing the Observation Space and Action Space\n",
    "        self.observation_space = spaces.Discrete(self.grid_size[0]* self.grid_size[1])\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.time_step = 0\n",
    "        \n",
    "        ## Actor Positions\n",
    "        self.bat_pos = [0,0]\n",
    "        self.robin_pos = [8,0]\n",
    "        self.selina_pos = (4,8)\n",
    "        \n",
    "        ## Negative Actors\n",
    "        self.joker_pos = (2,4)\n",
    "        self.arkham_pos = (4,3)\n",
    "        self.bane_pos = (1,8)\n",
    "        self.owl_pos = (4,7)\n",
    "        self.scarecrow_pos = (6,2)\n",
    "        \n",
    "        ## Postive Actors\n",
    "        self.batmobile_pos = (3,1)\n",
    "        self.redbird_pos = (6,3)\n",
    "        self.alfred_pos = ((1,3), (3,5), (7,6), (8,8))\n",
    "        # Load Images\n",
    "        self.batman_image = mpimg.imread('images/bat.png')\n",
    "        self.alfred_image = mpimg.imread('images/alfred.jpg')\n",
    "        self.robin_image = mpimg.imread('images/robin.jpg')\n",
    "        self.batmobile_image = mpimg.imread('images/batmobile.png')\n",
    "        self.redbird_image = mpimg.imread('images/red_bird.jpg')\n",
    "        \n",
    "        self.joker_image = mpimg.imread('images/joker.jpg')\n",
    "        self.arkham_image = mpimg.imread('images/arkham_asylum.jpg')\n",
    "        self.bane_image = mpimg.imread('images/bane.jpg')\n",
    "        self.bane_action = mpimg.imread('images/bane_breaks_bats.jpg')\n",
    "        self.owl_image = mpimg.imread('images/court_of_owls.jpg')\n",
    "        self.owl_action = mpimg.imread('images/court_of_owls_attack.png')\n",
    "        self.scarecrow_image = mpimg.imread('images/scare_crow.png')\n",
    "        self.scarecrow_action = mpimg.imread('images/scarecrow_attack.jpeg')\n",
    "        \n",
    "        self.selina_image = mpimg.imread('images/selina.png')\n",
    "        self.gotham_image = mpimg.imread('images/gotham.jpg')\n",
    "        \n",
    "        # Initializing the State of the environment.\n",
    "        self.state = np.zeros(self.grid_size)\n",
    "        self.state[tuple(self.bat_pos)] = 2\n",
    "        self.state[tuple(self.robin_pos)] = 1\n",
    "        self.state[tuple(self.selina_pos)] = 0.5\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.bat_pos = [0,0]\n",
    "        self.robin_pos = [8,0]\n",
    "        self.time_step = 0\n",
    "        \n",
    "        self.state = np.zeros(self.grid_size)\n",
    "        self.state[tuple(self.bat_pos)] = 2\n",
    "        self.state[tuple(self.robin_pos)] = 1\n",
    "        \n",
    "        observation = self.state.flatten()\n",
    "        \n",
    "        info = {}\n",
    "        info['Termination Message'] = 'Not Terminated'\n",
    "        \n",
    "        return self.bat_pos,self.robin_pos, info\n",
    "    \n",
    "    def get_reward(self, agent_pos, agent_old_pos, agent_name):\n",
    "\n",
    "        if np.array_equal(agent_pos, self.selina_pos):\n",
    "            # Assigining the reward of +20 on reaching the selina position.\n",
    "            return 20\n",
    "        elif np.array_equal(agent_pos, agent_old_pos):\n",
    "            # Assigining the reward of -1 on statying the same position after action.\n",
    "            return -1\n",
    "        elif np.array_equal(agent_pos, self.joker_pos):\n",
    "            # Assigining the reward of -10 on reaching the joker position.\n",
    "            return -10\n",
    "        elif np.array_equal(agent_pos, self.owl_pos):\n",
    "            # Assigining the reward of -7 on reaching the court of owls position.\n",
    "            return -7\n",
    "        elif np.array_equal(agent_pos, self.bane_pos):\n",
    "            # Assigining the reward of -5 on reaching the bane position.\n",
    "            return -5\n",
    "        elif np.array_equal(agent_pos, self.scarecrow_pos):\n",
    "            # Assigining the reward of -5 on reaching the bane position.\n",
    "            return -3\n",
    "        elif agent_pos in self.alfred_pos:\n",
    "            # Assigining the reward of -5 on reaching the Alfred position.\n",
    "            return 10\n",
    "        elif np.array_equal(agent_pos, self.arkham_pos):\n",
    "            # Assigining the reward of -1 on reaching the Arkham Asylum and moving the agent to starting position.\n",
    "            if agent_name == 'batman':\n",
    "                self.bat_pos = [0,0]\n",
    "            elif agent_name == 'robin':\n",
    "                self.robin_pos = [8,0]\n",
    "            return -1\n",
    "        elif np.array_equal(agent_pos, self.batmobile_pos):\n",
    "            # Assigining the reward of -1 on reaching the Arkham Asylum and moving the agent to starting position.\n",
    "            if agent_name == 'batman':\n",
    "                self.bat_pos = list(random.choice(self.alfred_pos))\n",
    "                return 10\n",
    "            elif agent_name == 'robin':\n",
    "                return 0\n",
    "        elif np.array_equal(agent_pos, self.redbird_pos):\n",
    "            # Assigining the reward of -1 on reaching the Arkham Asylum and moving the agent to starting position.\n",
    "            if agent_name == 'batman':\n",
    "                self.bat_pos = list(random.choice(self.alfred_pos))\n",
    "                return 0\n",
    "            elif agent_name == 'robin':\n",
    "                self.robin_pos = list(random.choice(self.alfred_pos))\n",
    "                return 10\n",
    "        else: return 0\n",
    "    \n",
    "    \n",
    "    def step(self, bat_action, robin_action):\n",
    "        reward_bat = 0\n",
    "        reward_robin = 0\n",
    "        bat_truncated = False\n",
    "        robin_truncated = False\n",
    "        self.state = np.zeros(self.grid_size)\n",
    "        # print('init_bat_pos', tuple(self.bat_pos) != self.selina_pos)\n",
    "        # print('init_robin_pos', tuple(self.robin_pos) != self.selina_pos)\n",
    "        if tuple(self.bat_pos) != self.selina_pos:\n",
    "            bat_old_pos = self.bat_pos.copy()\n",
    "            if bat_action == 0:\n",
    "                # Take Down action.\n",
    "                self.bat_pos[0] += 1\n",
    "            elif bat_action == 1:\n",
    "                # Take Up action.\n",
    "                self.bat_pos[0] -= 1\n",
    "            elif bat_action == 2:\n",
    "                # Take Right action.\n",
    "                self.bat_pos[1] += 1\n",
    "            elif bat_action == 3:\n",
    "                # Take Left action.\n",
    "                self.bat_pos[1] -= 1\n",
    "            \n",
    "            self.bat_pos[0] = np.clip(self.bat_pos[0], 0, self.grid_size[0]-1)\n",
    "            self.bat_pos[1] = np.clip(self.bat_pos[1], 0, self.grid_size[1]-1)\n",
    "            \n",
    "            reward_bat = self.get_reward(tuple(self.bat_pos), bat_old_pos, 'batman')\n",
    "            \n",
    "            self.state[tuple(self.bat_pos)] = 2\n",
    "            \n",
    "            if (self.bat_pos[0] >=0) & (self.bat_pos[0] <= self.grid_size[0]-1) & (self.bat_pos[1] >=0)  & (self.bat_pos[1] <= self.grid_size[1]-1):\n",
    "                bat_truncated = True\n",
    "            else:\n",
    "                bat_truncated = False\n",
    "        \n",
    "        if tuple(self.robin_pos) != self.selina_pos:\n",
    "            robin_old_pos = self.robin_pos.copy()\n",
    "            if robin_action == 0:\n",
    "                # Take Down action.\n",
    "                self.robin_pos[0] += 1\n",
    "            elif robin_action == 1:\n",
    "                # Take Up action.\n",
    "                self.robin_pos[0] -= 1\n",
    "            elif robin_action == 2:\n",
    "                # Take Right action.\n",
    "                self.robin_pos[1] += 1\n",
    "            elif robin_action == 3:\n",
    "                # Take Left action.\n",
    "                self.robin_pos[1] -= 1\n",
    "            \n",
    "            self.robin_pos[0] = np.clip(self.robin_pos[0], 0, self.grid_size[0]-1)\n",
    "            self.robin_pos[1] = np.clip(self.robin_pos[1], 0, self.grid_size[1]-1)\n",
    "            \n",
    "            reward_robin = self.get_reward(tuple(self.robin_pos), robin_old_pos, 'robin')\n",
    "            \n",
    "            self.state[tuple(self.robin_pos)] = 1\n",
    "            \n",
    "            if (self.robin_pos[0] >=0) & (self.robin_pos[0] <= self.grid_size[0]-1) & (self.robin_pos[1] >=0)  & (self.robin_pos[1] <= self.grid_size[1]-1):\n",
    "                robin_truncated = True\n",
    "            else:\n",
    "                robin_truncated = False\n",
    "        else:\n",
    "            reward_robin = 20\n",
    "        \n",
    "        self.state[tuple(self.selina_pos)] = 0.5\n",
    "        observation = self.state.flatten()\n",
    "        \n",
    "        # Updating the time step.\n",
    "        self.time_step += 1\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        # Updating the episode termination status.\n",
    "        if np.array_equal(self.bat_pos, self.robin_pos) and np.array_equal(self.bat_pos, self.selina_pos) and np.array_equal(self.robin_pos, self.selina_pos):\n",
    "            # Goal position is reached.\n",
    "            terminated = True\n",
    "            info['Termination Message'] = 'Goal Position Reached !!!'\n",
    "        elif self.time_step >= self.max_timesteps:\n",
    "            # Maximum time steps reached.\n",
    "            terminated = True\n",
    "            info['Termination Message'] = 'Maximum Time Reached'\n",
    "        else:\n",
    "            # Episode not terminated.\n",
    "            terminated = False\n",
    "            info['Termination Message'] = 'Not Terminated'\n",
    "            \n",
    "        return self.robin_pos,self.bat_pos, reward_bat, reward_robin, terminated, bat_truncated, robin_truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        fig , ax = plt.subplots()\n",
    "        \n",
    "        ax.imshow(self.gotham_image, extent=[0, self.grid_size[1], 0, self.grid_size[0]])\n",
    "        \n",
    "        ax.imshow(self.batman_image, extent=[self.bat_pos[1], self.bat_pos[1] + 1, self.grid_size[0] - self.bat_pos[0] - 1, self.grid_size[0] - self.bat_pos[0]])\n",
    "        ax.imshow(self.robin_image, extent=[self.robin_pos[1], self.robin_pos[1] + 1, self.grid_size[0] - self.robin_pos[0] - 1, self.grid_size[0] - self.robin_pos[0]])\n",
    "        \n",
    "        ax.imshow(self.batmobile_image, extent=[self.batmobile_pos[1], self.batmobile_pos[1] + 1, self.grid_size[0] - self.batmobile_pos[0] - 1, self.grid_size[0] - self.batmobile_pos[0]])\n",
    "        ax.imshow(self.redbird_image, extent=[self.redbird_pos[1], self.redbird_pos[1] + 1, self.grid_size[0] - self.redbird_pos[0] - 1, self.grid_size[0] - self.redbird_pos[0]])\n",
    "        \n",
    "        ax.imshow(self.joker_image, extent=[self.joker_pos[1], self.joker_pos[1] + 1, self.grid_size[0] - self.joker_pos[0] - 1, self.grid_size[0] - self.joker_pos[0]])\n",
    "        ax.imshow(self.arkham_image, extent=[self.arkham_pos[1], self.arkham_pos[1] + 1, self.grid_size[0] - self.arkham_pos[0] - 1, self.grid_size[0] - self.arkham_pos[0]])\n",
    "        \n",
    "        if np.array_equal(self.bat_pos, self.bane_pos) or np.array_equal(self.robin_pos, self.bane_pos):\n",
    "            ax.imshow(self.bane_action, extent=[self.bane_pos[1], self.bane_pos[1] + 1, self.grid_size[0] - self.bane_pos[0] - 1, self.grid_size[0] - self.bane_pos[0]])\n",
    "        else:\n",
    "            ax.imshow(self.bane_image, extent=[self.bane_pos[1], self.bane_pos[1] + 1, self.grid_size[0] - self.bane_pos[0] - 1, self.grid_size[0] - self.bane_pos[0]])\n",
    "        \n",
    "        if np.array_equal(self.bat_pos, self.owl_pos) or np.array_equal(self.robin_pos, self.owl_pos):\n",
    "            ax.imshow(self.owl_action, extent=[self.owl_pos[1], self.owl_pos[1] + 1, self.grid_size[0] - self.owl_pos[0] - 1, self.grid_size[0] - self.owl_pos[0]])\n",
    "        else:\n",
    "            ax.imshow(self.owl_image, extent=[self.owl_pos[1], self.owl_pos[1] + 1, self.grid_size[0] - self.owl_pos[0] - 1, self.grid_size[0] - self.owl_pos[0]])\n",
    "        \n",
    "        if np.array_equal(self.bat_pos, self.scarecrow_pos) or np.array_equal(self.robin_pos, self.scarecrow_pos):\n",
    "            ax.imshow(self.scarecrow_image, extent=[self.scarecrow_pos[1], self.scarecrow_pos[1] + 1, self.grid_size[0] - self.scarecrow_pos[0] - 1, self.grid_size[0] - self.scarecrow_pos[0]])\n",
    "        else:\n",
    "            ax.imshow(self.scarecrow_image, extent=[self.scarecrow_pos[1], self.scarecrow_pos[1] + 1, self.grid_size[0] - self.scarecrow_pos[0] - 1, self.grid_size[0] - self.scarecrow_pos[0]])\n",
    "        \n",
    "        ax.imshow(self.selina_image, extent=[self.selina_pos[1], self.selina_pos[1] + 1, self.grid_size[0] - self.selina_pos[0] - 1, self.grid_size[0] - self.selina_pos[0]])\n",
    "        \n",
    "        for pos in self.alfred_pos:\n",
    "            ax.imshow(self.alfred_image, extent=[pos[1], pos[1] + 1, self.grid_size[0] - pos[0] - 1, self.grid_size[0] - pos[0]])\n",
    "        ax.set_xlim(0, self.grid_size[1])\n",
    "        ax.set_ylim(0, self.grid_size[0])\n",
    "        \n",
    "        ax.axis('off')\n",
    "        # Save rendered image directly to a buffer\n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(buf, format='png', dpi=fig.dpi, bbox_inches='tight', pad_inches=0)\n",
    "        buf.seek(0)\n",
    "\n",
    "        # Convert buffer to PIL Image and then to RGB array\n",
    "        image = Image.open(buf)\n",
    "        rgb_image = np.array(image.convert('RGB'))\n",
    "        \n",
    "        plt.close(fig)  # Close the figure to prevent it from being displayed\n",
    "        return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Define neural network architecture for Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,128)\n",
    "        self.fc2 = nn.Linear(128,400)\n",
    "        self.fc3 = nn.Linear(400,64)\n",
    "        self.fc4 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Define replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Define MADDQN agent\n",
    "class MADDQNAgent:\n",
    "    def __init__(self, input_size, output_size, buffer_size, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.q_network = QNetwork(input_size, output_size).to(self.device)\n",
    "        self.target_q_network = QNetwork(input_size, output_size).to(self.device)\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    def select_action(self,state,train):\n",
    "        if train:\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                return random.randrange(self.output_size)\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(self.device)\n",
    "                q_values = self.q_network(state)\n",
    "                return torch.argmax(q_values).item()\n",
    "        else:\n",
    "             with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(self.device)\n",
    "                q_values = self.q_network(state)\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.add((state, action, reward, next_state, done))\n",
    "\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:\n",
    "            states, actions, rewards, next_states, dones = self.replay_buffer.sample(BATCH_SIZE)\n",
    "            states = torch.FloatTensor(states).to(self.device)\n",
    "            actions = torch.LongTensor(actions).to(self.device)\n",
    "            rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "            next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "            dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "            q_values = self.q_network(states)\n",
    "            next_q_values = self.target_q_network(next_states).detach()\n",
    "\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * torch.max(next_q_values, dim=1)[0]\n",
    "\n",
    "            q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            loss = self.loss(q_value, target_q_values)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment and hyperparameters\n",
    "env = GothamCityCooperative(1000)\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "INPUT_SIZE = 2\n",
    "OUTPUT_SIZE = env.action_space.n  # Assuming 4 actions (up, down, left, right)\n",
    "\n",
    "# Create two agents\n",
    "agent1 = MADDQNAgent(INPUT_SIZE, OUTPUT_SIZE, BUFFER_SIZE)\n",
    "agent2 = MADDQNAgent(INPUT_SIZE, OUTPUT_SIZE, BUFFER_SIZE)\n",
    "\n",
    "total_bat_reward = []\n",
    "total_robin_reward = []\n",
    "\n",
    "NUM_EPISODES = 1000\n",
    "# Training loop\n",
    "for episode in range(NUM_EPISODES):\n",
    "    # Initialize environment and other variables\n",
    "    done = False\n",
    "    bat_truncated = False\n",
    "    robin_truncated = False\n",
    "    episode_reward1 = 0\n",
    "    episode_reward2 = 0\n",
    "    state1,state2,_ = env.reset()\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        # Agent 1 selects action\n",
    "        action1 = agent1.select_action(state1,train=True)\n",
    "\n",
    "        # Agent 2 selects action\n",
    "        action2 = agent2.select_action(state2,train=True)\n",
    "\n",
    "        next_state1,next_state2,bat_reward,robin_reward,done,bat_truncated,robin_truncated,_ = env.step(action1, action2)\n",
    "\n",
    "        # Train Agent 1\n",
    "        agent1.train(state1, action1,bat_reward,next_state1, done)\n",
    "\n",
    "        # Train Agent 2\n",
    "        agent2.train(state2, action2,robin_reward, next_state1, done)\n",
    "\n",
    "        # Update states\n",
    "        state1 = next_state1\n",
    "        state2 = next_state2\n",
    "\n",
    "        # Update episode rewards\n",
    "        episode_reward1 += bat_reward\n",
    "        episode_reward2 += robin_reward\n",
    "\n",
    "        if step%4==0:\n",
    "            agent1.update_target_network()\n",
    "            agent2.update_target_network()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    total_bat_reward.append(episode_reward1)\n",
    "    total_robin_reward.append(episode_reward2)\n",
    "\n",
    "    print('Episode: ',episode,'Agent-1(Batman)reward:',episode_reward1,'Agent-2(Robin)reward:',episode_reward2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_bat_reward, label='Batman')\n",
    "plt.plot(total_robin_reward, label='Robin')\n",
    "plt.title('Rewards gained during training')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent1.target_q_network.state_dict(),'Agent-1_DDQN.pth')\n",
    "torch.save(agent2.target_q_network.state_dict(),'Agent-2_DDQN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent1 = MADDQNAgent(INPUT_SIZE,OUTPUT_SIZE,BUFFER_SIZE)\n",
    "# agent2 = MADDQNAgent(INPUT_SIZE,OUTPUT_SIZE,BUFFER_SIZE)\n",
    "agent1.q_network.load_state_dict(torch.load('Agent-1_DDQN.pth'))\n",
    "agent2.q_network.load_state_dict(torch.load('Agent-2_DDQN.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = MADDQNAgent(INPUT_SIZE,OUTPUT_SIZE,BUFFER_SIZE)\n",
    "agent2 = MADDQNAgent(INPUT_SIZE,OUTPUT_SIZE,BUFFER_SIZE)\n",
    "env = GothamCityCooperative(1000)\n",
    "test_bat_reward = []\n",
    "test_robin_reward = []\n",
    "for i in range(10):\n",
    "    state1,state2,_ = env.reset()\n",
    "    done = False\n",
    "    episode_bat_reward = 0\n",
    "    episode_robin_reward = 0\n",
    "    while not done:\n",
    "        action1 = agent1.select_action(state1,train=False)\n",
    "\n",
    "        # Agent 2 selects action\n",
    "        action2 = agent2.select_action(state2,train=False)\n",
    "        \n",
    "        next_state1,next_state2,bat_reward,robin_reward,done,bat_truncated,robin_truncated,_ = env.step(action1, action2)\n",
    "        state1 = next_state1\n",
    "        state2 = next_state2\n",
    "        episode_bat_reward += bat_reward\n",
    "        episode_robin_reward += robin_reward\n",
    "\n",
    "    test_bat_reward.append(episode_bat_reward)\n",
    "    test_robin_reward.append(episode_robin_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(test_bat_reward,label='Batman')\n",
    "plt.title('Testing MADDQN on Multi-agent Grid World environment')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(test_robin_reward,label='Robin')\n",
    "plt.title('Testing MADDQN on Multi-agent Grid World environment')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
